# Hugging Face Hosted Voice Assistant (Web Frontend + FastAPI Backend)

**Input:** Speech â†’ **STT**  
**LLM:** Text â†’ Text  
**Output:** Text on screen + **TTS** audio playback

No local models needed. Uses **Hugging Face Inference API** for STT, LLM, and TTS.

---

## ğŸ§± Architecture

```
[Browser Mic] â†’ /stt â†’ Whisper (HF API) â†’ transcript
transcript â†’ /chat â†’ LLM (HF API) â†’ reply text
reply text â†’ /tts â†’ TTS (HF API) â†’ WAV bytes â†’ play in browser
```

- **Frontend:** Plain HTML + JS (MediaRecorder API for mic, fetch() to backend)
- **Backend:** FastAPI with `huggingface-hub` `InferenceClient`
- **Auth:** Your HF token stays on the server (never in the browser)

---

## ğŸš€ Quick Start

### 1) Backend setup
```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# set your Hugging Face token (create at https://huggingface.co/settings/tokens)
export HF_TOKEN=hf_********************************    # PowerShell: $env:HF_TOKEN="hf_***"

# (Optional) choose different hosted models
export STT_MODEL="openai/whisper-small"
export LLM_MODEL="HuggingFaceH4/zephyr-7b-beta"
export TTS_MODEL="espnet/kan-bayashi_ljspeech_vits"

# run the API
uvicorn backend.app:app --reload --host 127.0.0.1 --port 8000
```

### 2) Frontend
Just open `frontend/index.html` in your browser.  
If your backend is not at `http://127.0.0.1:8000`, set it in DevTools console:
```js
localStorage.setItem("BACKEND_URL", "http://localhost:8000")
```

### 3) Use it
- Click **Start Recording**, speak, then **Stop**.  
- Click **Send to Assistant** â†’ it will show the transcript, generate a reply, and play the TTS audio.

---

## ğŸ”§ Notes & Customization

- **Security**: Keep `HF_TOKEN` only on server. Do **not** expose it to the client.
- **Audio format**: Browser records `audio/webm`; backend forwards bytes to HF STT. Most Whisper endpoints accept webm/ogg/wav/mp3.
- **Model choices** (env overrides):
  - STT: `openai/whisper-small`, `openai/whisper-medium`, etc.
  - LLM: `HuggingFaceH4/zephyr-7b-beta`, `mistralai/Mixtral-8x7B-Instruct`
  - TTS: `espnet/kan-bayashi_ljspeech_vits`, `facebook/fastspeech2-en-ljspeech`
- **Latency**: Larger models increase latency; start with smaller ones.
- **CORS**: Enabled for all origins by default (adjust for production).

---

## ğŸ§ª Troubleshooting

- **401 / auth errors** â†’ Ensure `HF_TOKEN` is set and valid.
- **CORS errors** â†’ Confirm backend URL matches your `localStorage BACKEND_URL` or keep defaults.
- **Mic permission** â†’ Allow microphone in the browser.
- **STT fails** â†’ Try shorter recordings (3â€“5s) and speak clearly; check model names.

---

## ğŸ“¦ Deploy

- Backend: Deploy FastAPI on a small VM or Platform-as-a-Service (Railway, Render, Azure Web App). Set env vars there.
- Frontend: Serve the static files from any static hosting (or same server as backend with a static route).
- Add HTTPS and restrict CORS origins to your domain in production.

---

## ğŸ” Secure Coding Reminders

- Never ship your HF token in the frontend.
- Rate-limit API endpoints if you put it on the internet.
- Log minimally (no raw audio, no secrets).
- Pin dependencies and update regularly.
